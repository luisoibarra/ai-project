% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage[hidelinks,breaklinks=true,backref=page]{hyperref}

\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Argumentación en Prensa Cubana}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Luis Ernesto Ibarra Vázquez\inst{1} \and
Luis Enrique Dalmau Coopat\inst{2} \and 
Adrián Hernández Pérez\inst{3}}
%
\authorrunning{L. Ibarra, L. Dalmau, A. Hernández}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Universidad de La Habana}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}

El proyecto se basa en el estudio de la argumentación en los periódicos cubanos. 
El objetivo principal es el análisis de las estructuras argumentativas que aparecen 
en ellos para conocer cuáles son las más usadas, cuáles son los argumentos expresados, 
entre otras estadísticas. En una primera etapa se concentrará en la segmentación y en 
la clasificación entre argumentos o no argumentos del texto, luego se clasificarán las
cláusulas según el rol que jueguen y por último se observará la relación existente 
entre dichas cláusulas.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%
%\section{First Section}
%\subsection{A Subsection Sample}
%Please note that the first paragraph of a section or subsection is
%not indented. The first paragraph that follows a table, figure,
%equation etc. does not need an indent, either.
%
%Subsequent paragraphs, however, are indented.
%
%\subsubsection{Sample Heading (Third Level)} Only two levels of
%headings should be numbered. Lower level headings remain unnumbered;
%they are formatted as run-in headings.
%
%\paragraph{Sample Heading (Fourth Level)}
%The contribution should contain no more than four levels of
%headings. Table~\ref{tab1} gives a summary of all heading levels.
%
%\begin{table}
%\caption{Table captions should be placed above the
%tables.}\label{tab1}
%\begin{tabular}{|l|l|l|}
%\hline
%Heading level &  Example & Font size and style\\
%\hline
%Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
%1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
%2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
%3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
%4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
%\hline
%\end{tabular}
%\end{table}
%
%
%\noindent Displayed equations are centered and set on a separate
%line.
%\begin{equation}
%x + y = z
%\end{equation}
%Please try to avoid rasterized images for line-art diagrams and
%schemas. Whenever possible, use vector graphics instead (see
%Fig.~\ref{fig1}).
%
%\begin{figure}
%%\includegraphics[width=\textwidth]{fig1.eps}
%\caption{A figure caption is always placed below the illustration.
%Please note that short captions are centered, while long ones are
%justified by the macro package automatically.} \label{fig1}
%\end{figure}
%
%\begin{theorem}
%This is a sample theorem. The run-in heading is set in bold, while
%the following text appears in italics. Definitions, lemmas,
%propositions, and corollaries are styled the same way.
%\end{theorem}
%%
%% the environments 'definition', 'lemma', 'proposition', 'corollary',
%% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%%
%\begin{proof}
%Proofs, examples, and remarks have the initial word in italics,
%while the following text appears in normal font.
%\end{proof}
%For citations of references, we prefer the use of square brackets
%and consecutive numbers. Citations using labels or the author/year
%convention are also acceptable. The following bibliography provides
%a sample reference list with entries for journal
%articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
%book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
%and a homepage~\cite{ref_url1}. Multiple citations are grouped
%\cite{ref_article1,ref_lncs1,ref_book1},
%\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%


\section{Introducción}

\section{Problema a resolver}

\section{Modelación}

\subsection{Seq2Seq}

\subsection{Convolución}
Estudios previos (Santos y Zadrozny, 2014; Chiu y Nichols, 2015) han demostrado que CNN es un enfoque eficaz para extraer información morfológica (como el prefijo o el sufijo de una palabra) de los caracteres de las palabras y codificarla en representaciones neuronales. La Figura 1 muestra la CNN que usamos para extraer la representación a nivel de carácter de una palabra dada. La CNN es similar a la de Chiu y Nichols (2015), excepto que usamos solo embeddings de caracteres como entradas a la CNN, sin características de tipo de carácter. Se aplica una capa de dropout (Srivastava et al., 2014) antes de que se ingresen las embeddings de caracteres en CNN.

%TODO INSERTAR FIGURA
Figura 1: La red neuronal de convolución para extraer representaciones de palabras a nivel de carácter. Las flechas discontinuas indican una capa de dropout aplicada antes de que se ingresen los embeddings de caracteres en CNN.


\subsection{LSTM}
	
Las redes neuronales recurrentes (RNN) son una poderosa familia de modelos conexionistas que capturan la dinámica del tiempo a través de ciclos en el grafo. Aunque, en teoría, las RNN son capaces de capturar dependencias de larga distancia, en la práctica fallan debido a los problemas de desaparición/explosión del gradiente (Bengio et al., 1994; Pascanu et al., 2012). Los LSTM (Hochreiter y Schmidhuber, 1997) son variantes de los RNN diseñados para hacer frente a estos problemas de desaparición de gradientes. Básicamente, una unidad LSTM se compone de tres puertas multiplicativas que controlan las proporciones de información para olvidar y pasar al siguiente paso de tiempo. La Figura 2 muestra la estructura básica de una unidad LSTM.

%TODO INSERTAR FIGURA 2
Figura 2: Esquema de una unidad LSTM

Formalmente las formulas para actualizar una unidad LSTM en un tiempo t son:

i t = σ(W i h t−1 + U i x t + b i )

f t = σ(W f h t−1 + U f x t + b f )

c̃ t = tanh(W c h t−1 + U c x t + b c )

c t = f t c t−1 + i t c̃ t

o t = σ(W o h t−1 + U o x t + b o )

h t = o t tanh(c t )

%TODO REVISAR LAS FORMULAS

donde σ es la funcion sigmoidal element-wise y CIRCLE SYMBOL es el producto element-wise. x_t es el  vector entrada (e.g. embeddings de palabras) en el tiempo t, y h_t es el vector de estado oculto (tambien llamado vector de salida) que guarda toda la informacion importante en (y antes) del tiempo  t. U_i , U_f , U_c , U_o denota las matrices de pesos de puertas difetentes para la entrada x_t , y W_i , W_f , W_c , W_o son las matrices de pesos para el estado oculto h_t. b_i , b_f , b_c , b_o denotan los vectores de bias. Cabe señalar que no se incluyen las conexiones de peephole (Gers et al., 2003) en esta formulación LSTM.

\subsubsection{BLSTM}
Para muchas tareas de etiquetado de secuencias, es beneficioso tener acceso a contextos pasados (izquierda) y futuros (derecha). Sin embargo, el estado oculto de LSTM h_t toma información solo del pasado, sin saber nada sobre el futuro. Una solución elegante cuya eficacia ha sido probada por trabajos anteriores (Dyer et al., 2015) es LSTM bidireccional (BLSTM). La idea básica es presentar cada secuencia hacia adelante y hacia atrás en dos estados ocultos separados para capturar información pasada y futura, respectivamente. Luego, los dos estados ocultos se concatenan para formar la salida final.

\subsection{CRF}
Para las tareas de etiquetado de secuencias (o predicción estructurada general), es beneficioso considerar las correlaciones entre las etiquetas en los vecindarios y decodificar conjuntamente la mejor cadena de etiquetas para una oración de entrada dada. Por ejemplo, en el etiquetado POS, es más probable que un adjetivo vaya seguido de un sustantivo que de un verbo, y en NER con anotación BIO2 estándar (Tjong Kim Sang y Veenstra, 1999), I-ORG no puede seguir a I-PER. Por lo tanto, modelamos la secuencia de etiquetas de forma conjunta utilizando un campo aleatorio condicional (CRF) (Lafferty et al., 2001), en lugar de decodificar cada etiqueta de forma independiente. Formalmente, usamos z = {z_1 , · · · , z_n } para representar una secuencia de entrada genérica donde z i es el vector de entrada de la i-ésima palabra. y = {y_1 , · · · , y_n } representa una secuencia genérica de etiquetas para z. Y(z) denota el conjunto de posibles secuencias de etiquetas para z. El modelo probabilístico para la secuencia CRF define una familia de probabilidad condicional p(y|z; W, b) sobre todas las posibles secuencias de etiquetas "y", dado "z" con la siguiente forma:

p(y|z; W, b) = \dfrac{\prod_{i=1}^{n}{ψ_i (y i−1 , y i , z)}}{\sum_{y_0 ∈ Y(z)}{\prod_{i=1}^{n}{ψ_i( (y_{i-1}',y_i' , z))}}}

%TODO REVISAR BIEN LAS FORMULAS

donde ψ_i(y_{i-1}' , y_{i}', z) = exp(W y T 0 ,y z i + b y 0 ,y ) son funciones potencia, u W_y T_0 ,"y" y  b_{y_{i-1}'} ,y son  el vector de pesos y bias correspondiente al par anotado (y' , y), respectivamente. Para el entrenamiento de la CRF , se usa la estimacion de la maxima verosimilitud condicional. Para un conjunto de entrenamiento {(z i , y i )}, el logaritmo de la verosimilitud (conocido como la log-verosimilitud) esta dada por:

L(W, b) =\sum_{0}i{log p(y|z; W, b)}

%TODO REVISAR TODAS LAS FORMULAS DEBEN ESTAR MAL REPRESENTADAS

El entrenamiento de la maxima verosimilitud escoge parametros tal que la log-verosimilitud es maximizada.

Decodificar es buscar la secuencia de la etiqueta y* con la mayor probabilidad condicional:

y ∗ = argmax_{y∈Y(z)} p(y|z; W, b)

Para un modelo CRF secuencial (solo se consideran las interacciones entre dos etiquetas sucesivas), el entrenamiento y la decodificación se pueden resolver de manera eficiente adoptando el algoritmo de Viterbi.
\subsection{BLSTM-CNNs-CRF}
	
Finalmente, se construye este modelo de red neuronal alimentando los vectores de salida de BLSTM en una capa CRF. La Figura 3 ilustra la arquitectura de esta red en detalle. Para cada palabra, la CNN calcula la representación a nivel de carácter en la Figura 1 con incrustaciones de caracteres como entradas. Luego, el vector de representación a nivel de carácter se concatena con el vector de inserción de palabras para alimentar la red BLSTM. Finalmente, los vectores de salida de BLSTM se alimentan a la capa CRF para decodificar conjuntamente la mejor secuencia de etiquetas. Como se muestra en la Figura 3, las capas de exclusión se aplican tanto en los vectores de entrada como de salida de BLSTM. Los resultados experimentales muestran que el uso de dropout mejora significativamente el rendimiento de este modelo

%TODO INSERTAR FIGURA 3
Figure 3: The main architecture of our neural
network. The character representation for each
word is computed by the CNN in Figure 1. Then
the character representation vector is concatenated
with the word embedding before feeding into the
BLSTM network. Dashed arrows indicate dropout
layers applied on both the input and output vectors
of BLSTM.



\section{Implementación}

\subsection{tensorflow}

\section{Entrenamiento}

\subsection{Corpus Usado}

\subsection{Hiperparámetros y optimización de estos}

\section{Evaluación}

\subsection{Resultados}

\section{Conslusiones}


\begin{thebibliography}{8}
\bibitem{ref_article1}
Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

\subsection{Long Short-Term Memory (LSTM)}

\subsection{Conditional Random Field (CRF)}


\section{Implementación}
\subsection{Tensor Flow}


\section{Entrenamiento}
\subsection{Corpus}
\subsection{ Hyperparametros y Optimización }

\section{Evaluación}
\section{Resultados}

\section{Concluciones}

\section{Referencias}



\bibitem{ref_url1}
LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
Oct 2017
\end{thebibliography}

\end{document}

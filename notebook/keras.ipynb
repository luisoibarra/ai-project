{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "params = {\n",
    "    'dim_chars': 100,\n",
    "    'dim': 300,\n",
    "    'dropout': 0.5,\n",
    "    'epochs': 25,\n",
    "    'batch_size': 20,\n",
    "    'filters': 30,\n",
    "    'kernel_size': 3,\n",
    "    'lstm_size': 200,\n",
    "    'optimizer': 'adam',\n",
    "    'metrics': ['acc'],\n",
    "    'loss': \"categorical_crossentropy\",\n",
    "    'model_path': str(Path(DATADIR, 'model.h5')),\n",
    "    'model_cnn_bilstm_crf_path': str(Path(DATADIR, 'model_cnn_bilstm_crf.h5')),\n",
    "    'model_cnn_bilstm_dn_crf_path': str(Path(DATADIR, 'model_cnn_bilstm_dn_crf.h5')),\n",
    "    'model_bilstm_crf_path': str(Path(DATADIR, 'model_bilstm_crf.h5')),\n",
    "    'model_bilstm_dn_crf_path': str(Path(DATADIR, 'model_bilstm_dn_crf.h5')),\n",
    "    'words_path': str(Path(DATADIR, 'vocab.words.txt')),\n",
    "    'chars_path': str(Path(DATADIR, 'vocab.chars.txt')),\n",
    "    'tags_path': str(Path(DATADIR, 'vocab.tags.txt')),\n",
    "    'sentences_path': (str(Path(DATADIR, 'train.words.txt')), str(Path(DATADIR, 'testa.words.txt')), str(Path(DATADIR, 'testb.words.txt'))),\n",
    "    'labels_path': (str(Path(DATADIR, 'train.tags.txt')), str(Path(DATADIR, 'testa.tags.txt')), str(Path(DATADIR, 'testb.tags.txt'))),\n",
    "    'glove_path': str(Path(DATADIR, 'glove.npz')),\n",
    "    'glove_raw_path': str(Path(DATADIR, 'glove.840B.300d.txt')),\n",
    "    'char_vectorization_path': str(Path(DATADIR, 'char_vectorization.npz'))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 10392\n",
      "Char amount 88\n",
      "Tag amount 10\n",
      "Max word size 19\n",
      "Max sentence size 79\n"
     ]
    }
   ],
   "source": [
    "def load_processed_files(params: dict):\n",
    "    # Loading preprocessed files\n",
    "\n",
    "    # Load vocab\n",
    "    with Path(params[\"words_path\"]).open() as f:\n",
    "        word_to_idx = {}\n",
    "        idx_to_word = {}\n",
    "        for idx, line in enumerate(f):\n",
    "            word = line.strip()\n",
    "            word_to_idx[word] = idx\n",
    "            idx_to_word[idx] = word\n",
    "\n",
    "    size_vocab = len(word_to_idx)\n",
    "    params['vocab_size'] = size_vocab\n",
    "    print(\"vocab_size\", size_vocab)\n",
    "\n",
    "    # Load chars\n",
    "    with Path(params[\"chars_path\"]).open() as f:\n",
    "        char_to_idx = {}\n",
    "        idx_to_char = []\n",
    "        for idx, line in enumerate(f):\n",
    "            char = line.strip()\n",
    "            char_to_idx[char] = idx\n",
    "            idx_to_char.append(char)\n",
    "\n",
    "    size_chars = len(char_to_idx)\n",
    "    print(\"Char amount\", size_chars)\n",
    "    params['char_amount'] = size_chars\n",
    "    params['chars'] = idx_to_char\n",
    "\n",
    "    # Load tags\n",
    "    with Path(params[\"tags_path\"]).open() as f:\n",
    "        tag_to_idx = {}\n",
    "        idx_to_tag = {}\n",
    "        for idx, line in enumerate(f):\n",
    "            tag = line.strip()\n",
    "            tag_to_idx[tag] = idx\n",
    "            idx_to_tag[idx] = tag\n",
    "    size_tags = len(tag_to_idx)\n",
    "    print(\"Tag amount\", size_tags)\n",
    "    params['tag_amount'] = size_tags\n",
    "\n",
    "\n",
    "    max_sent_size = 0\n",
    "    max_word_size = 0\n",
    "\n",
    "    for path in params[\"sentences_path\"]:\n",
    "        with Path(path).open() as f:\n",
    "            for sentence in f:\n",
    "                sentence = sentence.split(' ')\n",
    "                for word in sentence:\n",
    "                    max_word_size = max(max_word_size, len(word))\n",
    "                max_sent_size = max(max_sent_size, len(sentence))\n",
    "\n",
    "    print(\"Max word size\", max_word_size)\n",
    "    print(\"Max sentence size\", max_sent_size)\n",
    "\n",
    "    params['max_sent_size'] = max_sent_size\n",
    "    params['max_word_size'] = max_word_size\n",
    "\n",
    "load_processed_files(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset_string(params:dict):\n",
    "    # Loading dataset\n",
    "\n",
    "    train_sentences = [x for x in Path(params['sentences_path'][0]).read_text().splitlines()]\n",
    "    testa_sentences = [x for x in Path(params['sentences_path'][1]).read_text().splitlines()]\n",
    "    testb_sentences = [x for x in Path(params['sentences_path'][2]).read_text().splitlines()]\n",
    "\n",
    "    train_labels = [x.strip() for x in Path(params['labels_path'][0]).read_text().splitlines()]\n",
    "    testa_labels = [x.strip() for x in Path(params['labels_path'][1]).read_text().splitlines()]\n",
    "    testb_labels = [x.strip() for x in Path(params['labels_path'][2]).read_text().splitlines()]\n",
    "\n",
    "    params['train_sentences'] = train_sentences\n",
    "    params['testa_sentences'] = testa_sentences\n",
    "    params['testb_sentences'] = testb_sentences\n",
    "    \n",
    "    params['train_labels'] = train_labels\n",
    "    params['testa_labels'] = testa_labels\n",
    "    params['testb_labels'] = testb_labels\n",
    "    \n",
    "load_dataset_string(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   6  852  105 1850    5 2107]\n",
      "[  1 852   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0]\n",
      "[3 3 1 9 4 0]\n",
      "[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "[ 1  6 24  9  0  0]\n",
      "[46 43  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_vectorizers(params: dict):\n",
    "    \n",
    "    # Sentence vectorizer\n",
    "    vectorizer = layers.TextVectorization(\n",
    "        max_tokens=params[\"vocab_size\"], \n",
    "        output_sequence_length=params[\"max_sent_size\"]\n",
    "    )\n",
    "    \n",
    "    train_sentences = params['train_sentences']\n",
    "    testa_sentences = params['testa_sentences']\n",
    "    testb_sentences = params['testb_sentences']\n",
    "    \n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(train_sentences + testa_sentences + testb_sentences).batch(params[\"batch_size\"])\n",
    "    vectorizer.adapt(text_ds)\n",
    "    params['vectorizer_sentence'] = vectorizer\n",
    "\n",
    "    # Tags vectorizer\n",
    "    vectorizer_tags = layers.TextVectorization(\n",
    "        max_tokens=params[\"tag_amount\"],\n",
    "        output_sequence_length=params[\"max_sent_size\"]\n",
    "    )\n",
    "    \n",
    "    train_labels = params['train_labels']\n",
    "    testa_labels = params['testa_labels']\n",
    "    testb_labels = params['testb_labels']\n",
    "    \n",
    "    tags_ds = tf.data.Dataset.from_tensor_slices(train_labels + testa_labels + testb_labels).batch(params[\"batch_size\"])\n",
    "    vectorizer_tags.adapt(tags_ds)\n",
    "    \n",
    "    params['vectorizer_tags'] = vectorizer_tags\n",
    "    \n",
    "    \n",
    "    # Char vectorizer\n",
    "    vectorizer_chars = layers.TextVectorization(\n",
    "        max_tokens=params[\"char_amount\"],\n",
    "        output_sequence_length=params[\"max_word_size\"]\n",
    "    )\n",
    "    \n",
    "    chars = params['chars']\n",
    "    \n",
    "    vectorizer_chars.adapt(chars)\n",
    "    \n",
    "    params['vectorizer_chars'] = vectorizer_chars\n",
    "    \n",
    "    \n",
    "    # Checking vectorizer\n",
    "\n",
    "    # 0 -> Padding\n",
    "    # 1 -> Out Of Vocabulary\n",
    "\n",
    "    output = vectorizer([[\"los atletas les pagan en exceso\"]])\n",
    "    print(output.numpy()[0, :6])\n",
    "\n",
    "    output = vectorizer([[\"ascas atletas\"]])\n",
    "    print(output.numpy()[0, :])\n",
    "\n",
    "    output = vectorizer_tags([[\"O O a B-Claim I-Claim\"]])\n",
    "    print(output.numpy()[0, :6])\n",
    "\n",
    "    output = vectorizer_tags([[\"ascas atletas\"]])\n",
    "    print(output.numpy()[0, :])\n",
    "    \n",
    "    output = vectorizer_chars([[\"asv v d s\"]])\n",
    "    print(output.numpy()[0, :6])\n",
    "\n",
    "    output = vectorizer_chars([[\"/ 3 & 6\"]])\n",
    "    print(output.numpy()[0, :])\n",
    "    \n",
    "    \n",
    "    \n",
    "add_vectorizers(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 9824\n",
      "['', '[UNK]', 'de', 'la', 'que', 'en', 'los', 'y', 'a', 'el']\n",
      "Tag amount 10\n",
      "['', '[UNK]', 'ipremise', 'o', 'iclaim', 'imajorclaim', 'epremise', 'bpremise', 'eclaim', 'bclaim']\n",
      "Char amount 50\n",
      "['', '[UNK]', 'z', 'y', 'x', 'w', 'v', 'u', 't', 's', 'r', 'q', 'p', 'o', 'n', 'm', 'l', 'k', 'j', 'i', 'h', 'g', 'f', 'e', 'd', 'c', 'b', 'a', '\\u200b', 'ü', 'ú', 'ó', 'ñ', 'í', 'é', 'á', 'É', 'Á', '¿', '¡', '9', '8', '7', '6', '5', '4', '3', '2', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_word_to_index(params: dict):\n",
    "    vectorizer = params[\"vectorizer_sentence\"]\n",
    "    \n",
    "    # Index to word and Word to index\n",
    "    \n",
    "    # Sentences\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    params[\"words\"] = voc\n",
    "    params[\"word_index\"] = word_index\n",
    "\n",
    "    print(\"Vocab size\", len(voc))\n",
    "    print(params['words'][:10])\n",
    "    params['vocab_size'] = len(voc) - 2 # Minus PAD and UNK\n",
    "    \n",
    "    # Tags\n",
    "    vectorizer_tags = params[\"vectorizer_tags\"]\n",
    "    tags = vectorizer_tags.get_vocabulary()\n",
    "    tag_index = dict(zip(tags, range(len(tags))))\n",
    "    params[\"tags\"] = tags\n",
    "    params[\"tag_index\"] = tag_index\n",
    "\n",
    "    print(\"Tag amount\", len(tags))\n",
    "    print(params['tags'])\n",
    "    params['tag_amount'] = len(tags) - 2 # Minus PAD and UNK\n",
    "\n",
    "    # Chars\n",
    "    vectorizer_chars = params[\"vectorizer_chars\"]\n",
    "    chars = vectorizer_chars.get_vocabulary()\n",
    "    char_index = dict(zip(chars, range(len(chars))))\n",
    "    params[\"chars\"] = chars\n",
    "    params[\"char_index\"] = char_index\n",
    "\n",
    "    print(\"Char amount\", len(chars))\n",
    "    print(params['chars'])\n",
    "    params['char_amount'] = len(chars) - 2 # Minus PAD and UNK\n",
    "\n",
    "add_word_to_index(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Embedding Matrix Found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def loading_glove(params: dict):\n",
    "    \n",
    "    if Path(params[\"glove_path\"]).exists():\n",
    "        print(\"Glove Embedding Matrix Found\")\n",
    "        embedding_matrix = np.load(params[\"glove_path\"])[\"embeddings\"]\n",
    "        params['embedding_matrix'] = embedding_matrix\n",
    "        return\n",
    "    \n",
    "    # Loading Glove\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    num_tokens = len(params['words']) # Plus padding and unknown \n",
    "    embedding_dim = params['dim']\n",
    "    size_vocab = params['vocab_size']\n",
    "    word_index = params['word_index']\n",
    "\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    with Path(params[\"glove_raw_path\"]).open() as f:\n",
    "        for line_idx, line in enumerate(f):\n",
    "            if line_idx % 100000 == 0:\n",
    "                print('- At line {}'.format(line_idx))\n",
    "            line = line.strip().split()\n",
    "            if len(line) != 300 + 1:\n",
    "                continue\n",
    "            word = line[0]\n",
    "            embedding = line[1:]\n",
    "            if word in word_index:\n",
    "                hits += 1\n",
    "                word_idx = word_index[word]\n",
    "                embedding_matrix[word_idx] = embedding\n",
    "    print('- done. Found {} vectors for {} words'.format(hits, size_vocab))\n",
    "    \n",
    "    params['embedding_matrix'] = embedding_matrix\n",
    "    np.savez_compressed(params[\"glove_path\"], embeddings=embedding_matrix)\n",
    "\n",
    "loading_glove(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5102, 79)\n",
      "(5102, 79, 19)\n",
      "(5102, 79, 10)\n",
      "[ 112    5   84   10  166  876   71 4473   13    9  164    2    3  228\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n",
      "[[20 13  3 ...  0  0  0]\n",
      " [23 14  0 ...  0  0  0]\n",
      " [24 33 27 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_dataset_to_trainable_data(params: dict):\n",
    "    \n",
    "    # Sentences\n",
    "    \n",
    "    train_sentences = params['train_sentences']\n",
    "    testa_sentences = params['testa_sentences']\n",
    "    testb_sentences = params['testb_sentences']\n",
    "\n",
    "    vectorizer = params['vectorizer_sentence']\n",
    "    \n",
    "    x_train = vectorizer(np.array([[s] for s in train_sentences])).numpy()\n",
    "    x_test = vectorizer(np.array([[s] for s in testa_sentences])).numpy()\n",
    "    x_val = vectorizer(np.array([[s] for s in testb_sentences])).numpy()\n",
    "\n",
    "    params['x_train'] = x_train\n",
    "    params['x_test'] = x_test\n",
    "    params['x_val'] = x_val\n",
    "    \n",
    "    # Tags\n",
    "    \n",
    "    encoder = tf.keras.layers.CategoryEncoding(\n",
    "        num_tokens= len(params['tags']), # Plus padding, unk\n",
    "        output_mode=\"one_hot\"\n",
    "    )\n",
    "    \n",
    "    params[\"tag_encoder\"] = encoder\n",
    "        \n",
    "    train_labels = params['train_labels']\n",
    "    testa_labels = params['testa_labels']\n",
    "    testb_labels = params['testb_labels']\n",
    "    \n",
    "    vectorizer_tags = params['vectorizer_tags']\n",
    "    \n",
    "    y_train = vectorizer_tags(np.array([x for x in train_labels])).numpy()\n",
    "    y_test = vectorizer_tags(np.array([x for x in testa_labels])).numpy()\n",
    "    y_val = vectorizer_tags(np.array([x for x in testb_labels])).numpy()\n",
    "\n",
    "    y_train = np.array([encoder(x) for x in y_train])\n",
    "    y_test = np.array([encoder(x) for x in y_test])\n",
    "    y_val = np.array([encoder(x) for x in y_val])\n",
    "    \n",
    "    params[\"y_train\"] = y_train\n",
    "    params[\"y_test\"] = y_test\n",
    "    params[\"y_val\"] = y_val\n",
    "    \n",
    "    \n",
    "    # Chars\n",
    "    \n",
    "    char_vectorization_path = params[\"char_vectorization_path\"]\n",
    "    \n",
    "    if Path(char_vectorization_path).exists():\n",
    "        array_lists = np.load(char_vectorization_path)[\"embeddings\"]\n",
    "        \n",
    "        params['x_train_char'] = array_lists[\"train\"]\n",
    "        params['x_test_char'] = array_lists[\"test\"]\n",
    "        params['x_val_char'] = array_lists[\"val\"]\n",
    "    else:\n",
    "        vectorizer_chars = params[\"vectorizer_chars\"]\n",
    "        max_sent_size = params[\"max_sent_size\"]\n",
    "        def vetorize_char_sentence(sentence):\n",
    "            words = sentence.split(\" \")\n",
    "            word_padding = max_sent_size - len(words) \n",
    "            return vectorizer_chars([\" \".join(word) for word in words] + [\"\" for _ in range(word_padding)])\n",
    "\n",
    "        x_train_char = np.array(\n",
    "            [vetorize_char_sentence(sentence) for sentence in train_sentences])\n",
    "        x_test_char = np.array(\n",
    "            [vetorize_char_sentence(sentence) for sentence in testa_sentences])\n",
    "        x_val_char = np.array(\n",
    "            [vetorize_char_sentence(sentence) for sentence in testb_sentences])\n",
    "\n",
    "        params['x_train_char'] = x_train_char\n",
    "        params['x_test_char'] = x_test_char\n",
    "        params['x_val_char'] = x_val_char\n",
    "\n",
    "        np.savez_compressed(char_vectorization_path, train=x_train_char, test=x_test_char, val=x_val_char)\n",
    "    \n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(x_train_char.shape)\n",
    "    print(y_train.shape)\n",
    "    print(x_train[0])\n",
    "    print(x_train_char[0])\n",
    "    print(y_train[0])\n",
    "\n",
    "convert_dataset_to_trainable_data(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model_bilstm_crf(params: dict):\n",
    "\n",
    "    words_amount = len(params['words']) # Plus padding and unknown \n",
    "    embedding_dim = params['dim']\n",
    "    embedding_matrix = params['embedding_matrix']\n",
    "    max_sentence_size = params['max_sent_size']\n",
    "    lstm_size = params[\"lstm_size\"]\n",
    "    tag_amount = len(params['tags']) # Plus padding and unknown \n",
    "    optimizer = params['optimizer']\n",
    "    metrics = params['metrics']\n",
    "    dropout = params['dropout']\n",
    "    loss = params['loss']\n",
    "\n",
    "    # Input layer\n",
    "    int_sequences_input = keras.Input(shape=(max_sentence_size,), dtype=\"int64\")\n",
    "    \n",
    "    # Embedding layer, convert an index vector into a embedding vector, by accessing embedding_matrix\n",
    "    embedding_layer = layers.Embedding(\n",
    "        words_amount,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_sentence_size\n",
    "    )\n",
    "\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "    # BiLSTM Layer\n",
    "    model_layers = layers.Bidirectional(layers.LSTM(\n",
    "        lstm_size, \n",
    "        return_sequences=True))(embedded_sequences)\n",
    "\n",
    "    # Dropout layer\n",
    "    model_layers = layers.Dropout(dropout)(model_layers)\n",
    "    \n",
    "    # CRF Layer\n",
    "    crf = tfa.layers.CRF(tag_amount)\n",
    "    decoded_sequence, potentials, sequence_length, chain_kernel = crf(model_layers)\n",
    "\n",
    "    # Model\n",
    "    model = keras.Model(int_sequences_input, potentials)\n",
    "    model.compile(\n",
    "        loss=loss, \n",
    "        optimizer=optimizer, \n",
    "        metrics=metrics)\n",
    "    model.summary()\n",
    "    \n",
    "    params[\"model\"] = model\n",
    "    params[\"model_bilstm_crf\"] = model\n",
    "\n",
    "create_model_bilstm_crf(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_bilstm_dn_crf(params: dict):\n",
    "\n",
    "    words_amount = len(params['words']) # Plus padding and unknown \n",
    "    embedding_dim = params['dim']\n",
    "    embedding_matrix = params['embedding_matrix']\n",
    "    max_sentence_size = params['max_sent_size']\n",
    "    lstm_size = params[\"lstm_size\"]\n",
    "    tag_amount = len(params['tags']) # Plus padding and unknown \n",
    "    optimizer = params['optimizer']\n",
    "    metrics = params['metrics']\n",
    "    dropout = params['dropout']\n",
    "    loss = params['loss']\n",
    "\n",
    "    # Input layer\n",
    "    int_sequences_input = keras.Input(shape=(max_sentence_size,), dtype=\"int64\")\n",
    "    \n",
    "    # Embedding layer, convert an index vector into a embedding vector, by accessing embedding_matrix\n",
    "    embedding_layer = layers.Embedding(\n",
    "        words_amount,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_sentence_size\n",
    "    )\n",
    "\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "    # BiLSTM Layer\n",
    "    model_layers = layers.Bidirectional(layers.LSTM(\n",
    "        lstm_size, \n",
    "        return_sequences=True))(embedded_sequences)\n",
    "  \n",
    "    # Dropout layer\n",
    "    model_layers = layers.Dropout(dropout)(model_layers) # TODO This layer isn't in the original model\n",
    "   \n",
    "    # Dense layer\n",
    "    model_layers = layers.TimeDistributed(layers.Dense(50))(model_layers) # TODO This layer isn't in the original model\n",
    "    \n",
    "    # Dropout layer\n",
    "    model_layers = layers.Dropout(dropout)(model_layers)\n",
    "    \n",
    "    # CRF Layer\n",
    "    crf = tfa.layers.CRF(tag_amount)\n",
    "    decoded_sequence, potentials, sequence_length, chain_kernel = crf(model_layers)\n",
    "\n",
    "    # Model\n",
    "    model = keras.Model(int_sequences_input, potentials)\n",
    "    model.compile(\n",
    "        loss=loss\n",
    "        optimizer=optimizer, \n",
    "        metrics=metrics)\n",
    "    model.summary()\n",
    "    \n",
    "    params[\"model\"] = model\n",
    "    params[\"model_bilstm_dn_crf\"] = model\n",
    "\n",
    "create_model_bilstm_dn_crf(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_save_model_no_cnn(params: dict, model_name='model'):\n",
    "    \n",
    "    epochs = params['epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    model = params[model_name]\n",
    "    x_train, y_train = params['x_train'], params['y_train']\n",
    "    x_val, y_val = params['x_val'], params['y_val']\n",
    "    \n",
    "    model.fit(x_train, y_train, \n",
    "              batch_size=params[\"batch_size\"], \n",
    "              epochs=params['epochs'], \n",
    "              validation_data=(x_val, y_val))\n",
    "    \n",
    "    model.save(params[f\"{model_name}_path\"])\n",
    "\n",
    "model_name = \"model_bilstm_dn_crf\"\n",
    "model_name = \"model_bilstm_crf\"\n",
    "    \n",
    "train_and_save_model_no_cnn(params, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def evaluate_model_no_cnn(params: dict, model_name='model'):\n",
    "    batch_size = params['batch_size']\n",
    "    model = params[model_name]\n",
    "    x_test, y_test = params['x_test'], params['y_test']\n",
    "    \n",
    "    results = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print(\"test loss, test acc\", results)\n",
    "\n",
    "model_name = \"model_bilstm_dn_crf\"\n",
    "model_name = \"model_bilstm_crf\"\n",
    "\n",
    "evaluate_model_no_cnn(params, model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_cnn_bilstm_dn_crf(params: dict):\n",
    "\n",
    "    words_amount = len(params['words']) # Plus padding and unknown \n",
    "    embedding_dim = params['dim']\n",
    "    embedding_matrix = params['embedding_matrix']\n",
    "    max_sentence_size = params['max_sent_size']\n",
    "    lstm_size = params[\"lstm_size\"]\n",
    "    tag_amount = len(params['tags']) # Plus padding and unknown \n",
    "    optimizer = params['optimizer']\n",
    "    metrics = params['metrics']\n",
    "    dropout = params['dropout']\n",
    "    loss = params[\"loss\"]\n",
    "    \n",
    "    dim_chars = params[\"dim_chars\"]\n",
    "    filters = params['filters']\n",
    "    kernel_size = params[\"kernel_size\"]\n",
    "    max_word_size = params['max_word_size']\n",
    "    batch_size = params['batch_size']\n",
    "    char_index = params['char_index']\n",
    "    chars = params['chars']\n",
    "    char_amount = len(chars) # Plus padding and unknown\n",
    "    \n",
    "    # Input layer char\n",
    "    int_char_input = keras.Input(shape=(max_word_size,), dtype=\"int64\")\n",
    "    \n",
    "    # Embedding layer char\n",
    "    embedding_char_layer = layers.Embedding(\n",
    "        char_amount,\n",
    "        dim_chars,\n",
    "        trainable=True,\n",
    "        input_length=max_word_size\n",
    "    )\n",
    "    \n",
    "    embedded_chars = embedding_char_layer(int_char_input)\n",
    "    \n",
    "    # Dropout layer char\n",
    "    model_char_layers = layers.Dropout(dropout)(embedded_chars)\n",
    "   \n",
    "    # Convolution Layer char\n",
    "    \n",
    "    model_char_layers = layers.Conv1D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        padding=\"same\",\n",
    "        input_shape=(max_word_size, dim_chars)\n",
    "    )(model_char_layers)\n",
    "    \n",
    "    # Max Polling layer char\n",
    "\n",
    "    model_char_layers = layers.MaxPooling1D(\n",
    "        max_word_size,\n",
    "        input_shape=(max_word_size, filters)\n",
    "    )(model_char_layers)\n",
    "    \n",
    "    # Reshape layer char\n",
    "    \n",
    "    model_char_layers = layers.Reshape((filters,), input_shape=(1, filters))(model_char_layers)\n",
    "    \n",
    "    char_model = keras.Model(int_char_input, model_char_layers)\n",
    "    \n",
    "    char_model.summary()\n",
    "    \n",
    "    # Time Distributed with words\n",
    "    inputs_word_chars = keras.Input(shape=(max_sentence_size, max_word_size), dtype=\"int64\")\n",
    "    model_char_layers = layers.TimeDistributed(char_model)(inputs_word_chars)\n",
    "    \n",
    "    # Dropout layer\n",
    "    model_char_layers = layers.Dropout(dropout)(model_char_layers)\n",
    "   \n",
    "    \n",
    "    # Input layer\n",
    "    int_sequences_input = keras.Input(shape=(max_sentence_size,), dtype=\"int64\")\n",
    "    \n",
    "    # Embedding layer, convert an index vector into a embedding vector, by accessing embedding_matrix\n",
    "    embedding_layer = layers.Embedding(\n",
    "        words_amount,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_sentence_size\n",
    "    )\n",
    "\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "    \n",
    "    # Concatenate char embedding with sentence embedding\n",
    "    model_layers = layers.Concatenate()([embedded_sequences, model_char_layers])\n",
    "    \n",
    "    # BiLSTM Layer\n",
    "    model_layers = layers.Bidirectional(layers.LSTM(\n",
    "        lstm_size, \n",
    "        return_sequences=True))(model_layers)\n",
    "  \n",
    "    # Dropout layer\n",
    "    model_layers = layers.Dropout(dropout)(model_layers) # TODO This layer isn't in the original model\n",
    "   \n",
    "    # Dense layer\n",
    "    model_layers = layers.TimeDistributed(layers.Dense(50))(model_layers) # TODO This layer isn't in the original model\n",
    "\n",
    "    # Dropout layer\n",
    "    model_layers = layers.Dropout(dropout)(model_layers)\n",
    "    \n",
    "    # CRF Layer\n",
    "    crf = tfa.layers.CRF(tag_amount)\n",
    "    decoded_sequence, potentials, sequence_length, chain_kernel = crf(model_layers)\n",
    "\n",
    "    # Model\n",
    "    model = keras.Model([int_sequences_input, inputs_word_chars], potentials)\n",
    "    model.compile(\n",
    "        loss=loss\n",
    "        optimizer=optimizer, \n",
    "        metrics=metrics)\n",
    "    model.summary()\n",
    "    \n",
    "    params[\"model\"] = model\n",
    "    params[\"model_cnn_bilstm_dn_crf\"] = model\n",
    "\n",
    "create_model_cnn_bilstm_dn_crf(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_cnn_bilstm_crf(params: dict):\n",
    "\n",
    "    words_amount = len(params['words']) # Plus padding and unknown \n",
    "    embedding_dim = params['dim']\n",
    "    embedding_matrix = params['embedding_matrix']\n",
    "    max_sentence_size = params['max_sent_size']\n",
    "    lstm_size = params[\"lstm_size\"]\n",
    "    tag_amount = len(params['tags']) # Plus padding and unknown \n",
    "    optimizer = params['optimizer']\n",
    "    metrics = params['metrics']\n",
    "    dropout = params['dropout']\n",
    "    loss = params[\"loss\"]\n",
    "    \n",
    "    dim_chars = params[\"dim_chars\"]\n",
    "    filters = params['filters']\n",
    "    kernel_size = params[\"kernel_size\"]\n",
    "    max_word_size = params['max_word_size']\n",
    "    batch_size = params['batch_size']\n",
    "    char_index = params['char_index']\n",
    "    chars = params['chars']\n",
    "    char_amount = len(chars) # Plus padding and unknown\n",
    "    \n",
    "    # Input layer char\n",
    "    int_char_input = keras.Input(shape=(max_word_size,), dtype=\"int64\")\n",
    "    \n",
    "    # Embedding layer char\n",
    "    embedding_char_layer = layers.Embedding(\n",
    "        char_amount,\n",
    "        dim_chars,\n",
    "        trainable=True,\n",
    "        input_length=max_word_size\n",
    "    )\n",
    "    \n",
    "    embedded_chars = embedding_char_layer(int_char_input)\n",
    "    \n",
    "    # Dropout layer char\n",
    "    model_char_layers = layers.Dropout(dropout)(embedded_chars)\n",
    "   \n",
    "    # Convolution Layer char\n",
    "    \n",
    "    model_char_layers = layers.Conv1D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        padding=\"same\",\n",
    "        input_shape=(max_word_size, dim_chars)\n",
    "    )(model_char_layers)\n",
    "    \n",
    "    # Max Polling layer char\n",
    "\n",
    "    model_char_layers = layers.MaxPooling1D(\n",
    "        max_word_size,\n",
    "        input_shape=(max_word_size, filters)\n",
    "    )(model_char_layers)\n",
    "    \n",
    "    # Reshape layer char\n",
    "    \n",
    "    model_char_layers = layers.Reshape((filters,), input_shape=(1, filters))(model_char_layers)\n",
    "    \n",
    "    char_model = keras.Model(int_char_input, model_char_layers)\n",
    "    \n",
    "    char_model.summary()\n",
    "    \n",
    "    # Time Distributed with words\n",
    "    inputs_word_chars = keras.Input(shape=(max_sentence_size, max_word_size), dtype=\"int64\")\n",
    "    model_char_layers = layers.TimeDistributed(char_model)(inputs_word_chars)\n",
    "    \n",
    "    # Dropout layer\n",
    "    model_char_layers = layers.Dropout(dropout)(model_char_layers)\n",
    "   \n",
    "    \n",
    "    # Input layer\n",
    "    int_sequences_input = keras.Input(shape=(max_sentence_size,), dtype=\"int64\")\n",
    "    \n",
    "    # Embedding layer, convert an index vector into a embedding vector, by accessing embedding_matrix\n",
    "    embedding_layer = layers.Embedding(\n",
    "        words_amount,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "        input_length=max_sentence_size\n",
    "    )\n",
    "\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "    \n",
    "    # Concatenate char embedding with sentence embedding\n",
    "    model_layers = layers.Concatenate()([embedded_sequences, model_char_layers])\n",
    " \n",
    "    # BiLSTM Layer\n",
    "    model_layers = layers.Bidirectional(layers.LSTM(\n",
    "        lstm_size, \n",
    "        return_sequences=True))(model_layers)\n",
    "  \n",
    "    # Dropout layer\n",
    "    model_layers = layers.Dropout(dropout)(model_layers)\n",
    "    \n",
    "    # CRF Layer\n",
    "    crf = tfa.layers.CRF(tag_amount)\n",
    "    decoded_sequence, potentials, sequence_length, chain_kernel = crf(model_layers)\n",
    "\n",
    "    # Model\n",
    "    model = keras.Model([int_sequences_input, inputs_word_chars], potentials)\n",
    "    model.compile(\n",
    "        loss=loss\n",
    "        optimizer=optimizer, \n",
    "        metrics=metrics)\n",
    "    model.summary()\n",
    "    \n",
    "    params[\"model\"] = model\n",
    "    params[\"model_cnn_bilstm_crf\"] = model\n",
    "\n",
    "create_model_cnn_bilstm_crf(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_save_model_cnn(params: dict, model_name='model'):\n",
    "    \n",
    "    epochs = params['epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    model = params[model_name]\n",
    "    x_train, x_train_char, y_train = params['x_train'], params['x_train_char'], params['y_train']\n",
    "    x_val, x_val_char, y_val = params['x_val'], params['x_val_char'], params['y_val']\n",
    "    \n",
    "    model.fit([x_train, x_train_char], y_train, \n",
    "              batch_size=params[\"batch_size\"], \n",
    "              epochs=params['epochs'], \n",
    "              validation_data=([x_val, x_val_char], y_val))\n",
    "    \n",
    "    model.save(params[f\"{model_name}_path\"])\n",
    "\n",
    "model_name = \"model_cnn_bilstm_dn_crf\"\n",
    "model_name = \"model_cnn_bilstm_crf\"\n",
    "\n",
    "train_and_save_model_cnn(params, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def evaluate_model_cnn(params: dict, model_name='model'):\n",
    "    batch_size = params['batch_size']\n",
    "    model = params[model_name]\n",
    "    x_test, x_test_char, y_test = params['x_test'], params['x_test_char'], params['y_test']\n",
    "    \n",
    "    results = model.evaluate([x_test, x_test_char], y_test, batch_size=batch_size)\n",
    "    print(\"test loss, test acc\", results)\n",
    "\n",
    "model_name = \"model_cnn_bilstm_dn_crf\"\n",
    "model_name = \"model_cnn_bilstm_crf\"\n",
    "\n",
    "evaluate_model_cnn(params, model_name)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

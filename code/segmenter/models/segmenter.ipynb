{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Segmentation Model\n",
    "\n",
    "## TODO\n",
    "\n",
    "- [ ] Spanish Embeddings\n",
    "- [ ] Investigate Metrics. The Accuracy metric is giving wrong results. Maybe is measuring the entire sequence match and not intrer sequence match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INFO_TAG = \"persuasive_essays_paragraph\"\n",
    "\n",
    "SOURCE_DATADIR = Path(f\"../../data/projection/{INFO_TAG}\").resolve()\n",
    "DATA_DIR = Path(f\"../../data/segmenter_corpus/{INFO_TAG}\").resolve()\n",
    "TO_PROCESS_DATADIR = Path(f\"../../data/to_process\").resolve()\n",
    "PROCESSED_DATADIR = Path(f\"../../data/processed/{INFO_TAG}\").resolve()\n",
    "GLOVE_DIR = Path(\"../../data/\", 'glove.840B.300d.txt')\n",
    "\n",
    "params = {\n",
    "    # Model Hyperparameters\n",
    "    'dim': 300,\n",
    "    'dropout': 0.5,\n",
    "    'epochs': 70,\n",
    "    'batch_size': 20,\n",
    "    'lstm_size': 200,\n",
    "    'optimizer': 'adam',\n",
    "    'metrics': ['acc'],\n",
    "    'trainable_word_emeddings': True,\n",
    "    \n",
    "    # CNN Char Hyperparameters\n",
    "    'dim_chars': 100,\n",
    "    'filters': 30,\n",
    "    'kernel_size': 3,\n",
    "    \n",
    "    # Dense Layer Hyperparameters\n",
    "    'dense_units': 50,\n",
    "    'dense_activation': \"relu\",\n",
    "    \n",
    "    # Vectorizer Hyperparameters\n",
    "    'standardize': None, \n",
    "    'split': \"whitespace\",\n",
    "\n",
    "    # Model configuration\n",
    "    'with_cnn': True,\n",
    "    'with_dn': False,\n",
    "    \n",
    "    # Corpus info\n",
    "#     'corpus_type': \"sentence\",\n",
    "    'corpus_type': \"paragraph\", \n",
    "    'language': \"english\",\n",
    "#     'meta_tags_level': 0, # BIOES \n",
    "    'meta_tags_level': 1, # BIOES-MetaTag\n",
    "    'words_path': str(Path(DATA_DIR, 'vocab.words.txt')),\n",
    "    'chars_path': str(Path(DATA_DIR, 'vocab.chars.txt')),\n",
    "    'tags_path': str(Path(DATA_DIR, 'vocab.tags.txt')),\n",
    "    'sequences_path': (str(Path(DATA_DIR, 'train.words.txt')), str(Path(DATA_DIR, 'testa.words.txt')), str(Path(DATA_DIR, 'testb.words.txt'))),\n",
    "    'labels_path': (str(Path(DATA_DIR, 'train.tags.txt')), str(Path(DATA_DIR, 'testa.tags.txt')), str(Path(DATA_DIR, 'testb.tags.txt'))),\n",
    "    'glove_path': str(Path(DATA_DIR, 'glove.npz')),\n",
    "    'glove_raw_path': str(Path(GLOVE_DIR)),\n",
    "    'char_vectorization_path': str(Path(DATA_DIR, 'char_vectorization.npz')),\n",
    "\n",
    "    # Data info\n",
    "    'source_data_path': str(Path(SOURCE_DATADIR)), # Directory with conll annotated texts\n",
    "    'data_path': str(Path(DATA_DIR)), # Directory with the segmenter corpus\n",
    "    'to_process_data_path': str(Path(TO_PROCESS_DATADIR)), # Directory with text to be processed\n",
    "    'processed_data_path': str(Path(PROCESSED_DATADIR)), # Directory to save the processed data\n",
    "}\n",
    "\n",
    "MODEL_NAME = params['corpus_type'] + \"_model\" + (\"_cnn\" if params['with_cnn'] else \"\") + \"_blstm\" + (\"_dn\" if params['with_dn'] else \"\") + \"_crf\"\n",
    "\n",
    "params.update({\n",
    "    'model_name': MODEL_NAME,\n",
    "    \n",
    "    # Model serialization info\n",
    "    'model_path': str(Path(DATA_DIR, MODEL_NAME)),\n",
    "    'full_model_path': str(Path(DATA_DIR, MODEL_NAME)),\n",
    "    'model_histories_path': str(Path(DATA_DIR, MODEL_NAME)),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Corpus\n",
    "\n",
    "Creates the corpus for the model based on the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare model corpus\n",
    "from segmenter_exporter import export_directory, export_files\n",
    "\n",
    "\n",
    "def create_segmenter_corpus(params: dict):\n",
    "    corpus_type = params['corpus_type']\n",
    "    if corpus_type == \"paragraph\":\n",
    "        export_directory(Path(params['source_data_path']), \n",
    "                         Path(params['data_path']), \n",
    "                         language=params['language'],\n",
    "                         meta_tags_level=params['meta_tags_level'])\n",
    "    elif corpus_type == \"sentence\":\n",
    "        # For sentences\n",
    "        export_files(Path(params['source_data_path']), \n",
    "                     Path(params['data_path']), \n",
    "                     language=params['language'],\n",
    "                     meta_tags_level=params['meta_tags_level'])\n",
    "    else:\n",
    "        print(f\"WARNING: {corpus_type} is an invalid corpus_type\")\n",
    "\n",
    "create_segmenter_corpus(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Vectorizers\n",
    "\n",
    "Add the datasets and vectorizers. Also other values are computed such as char amount, word amount, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_amount 11\n",
      "char_amount 89\n",
      "word_amount 9007\n",
      "max_word_size 19\n",
      "max_seq_size 577\n",
      "\n",
      "word_to_index\n",
      "Length: 9009\n",
      "First 20: [('', 0), ('[UNK]', 1), (',', 2), ('de', 3), ('.', 4), ('la', 5), ('que', 6), ('los', 7), ('en', 8), ('y', 9), ('a', 10), ('el', 11), ('las', 12), ('para', 13), ('un', 14), ('es', 15), ('una', 16), ('m√°s', 17), ('no', 18), ('se', 19)]\n",
      "\n",
      "tag_to_index\n",
      "Length: 13\n",
      "First 20: [('', 0), ('[UNK]', 1), ('I-Premise', 2), ('O', 3), ('I-Claim', 4), ('I-MajorClaim', 5), ('E-Premise', 6), ('B-Premise', 7), ('E-Claim', 8), ('B-Claim', 9), ('E-MajorClaim', 10), ('B-MajorClaim', 11), ('S-MajorClaim', 12)]\n",
      "\n",
      "char_to_index\n",
      "Length: 91\n",
      "First 20: [('', 0), ('[UNK]', 1), (' ', 2), ('e', 3), ('a', 4), ('s', 5), ('o', 6), ('n', 7), ('r', 8), ('i', 9), ('d', 10), ('l', 11), ('t', 12), ('u', 13), ('c', 14), ('m', 15), ('p', 16), ('b', 17), (',', 18), ('g', 19)]\n"
     ]
    }
   ],
   "source": [
    "def add_datasets(params: dict):\n",
    "    \n",
    "    def configure_dataset(dataset):\n",
    "        return dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    def get_length_dataset(dataset):\n",
    "        return int(dataset.reduce(initial_state=0, reduce_func=lambda x,y: x + 1))\n",
    "    \n",
    "    # Creating datasets\n",
    "    \n",
    "    train_sequences_dataset = configure_dataset(tf.data.TextLineDataset([params['sequences_path'][0]]))\n",
    "    testa_sequences_dataset = configure_dataset(tf.data.TextLineDataset([params['sequences_path'][1]]))\n",
    "    testb_sequences_dataset = configure_dataset(tf.data.TextLineDataset([params['sequences_path'][2]]))\n",
    "    \n",
    "    train_labels_dataset = configure_dataset(tf.data.TextLineDataset([params['labels_path'][0]]))\n",
    "    testa_labels_dataset = configure_dataset(tf.data.TextLineDataset([params['labels_path'][1]]))\n",
    "    testb_labels_dataset = configure_dataset(tf.data.TextLineDataset([params['labels_path'][2]]))\n",
    "    \n",
    "    chars_dataset = configure_dataset(tf.data.TextLineDataset([params['chars_path']]))\n",
    "    words_dataset = configure_dataset(tf.data.TextLineDataset([params['words_path']]))\n",
    "    tags_dataset = configure_dataset(tf.data.TextLineDataset([params['tags_path']]))\n",
    "    \n",
    "    # Saving datasets\n",
    "    \n",
    "    params[\"train_sequences\"] = train_sequences_dataset\n",
    "    params[\"testa_sequences\"] = testa_sequences_dataset\n",
    "    params[\"testb_sequences\"] = testb_sequences_dataset\n",
    "    \n",
    "    params[\"train_labels\"] = train_labels_dataset\n",
    "    params[\"testa_labels\"] = testa_labels_dataset\n",
    "    params[\"testb_labels\"] = testb_labels_dataset\n",
    "    \n",
    "    params[\"chars\"] = chars_dataset\n",
    "    params[\"words\"] = words_dataset\n",
    "    params[\"tags\"] = tags_dataset\n",
    "    \n",
    "    # Computing dataset information\n",
    "    \n",
    "    params['tag_amount'] = get_length_dataset(tags_dataset)\n",
    "    params['char_amount'] = get_length_dataset(chars_dataset)\n",
    "    params['word_amount'] = get_length_dataset(words_dataset)\n",
    "    params['max_word_size'] = int(words_dataset.reduce(initial_state=0, reduce_func=lambda x,y: tf.maximum(x, tf.strings.length(y))))\n",
    "    params['max_seq_size'] = int(train_labels_dataset.reduce(initial_state=tf.constant([0]), reduce_func=lambda x,y: tf.maximum(x, tf.shape(tf.strings.split(y, sep=\" \"))))[0])\n",
    "    \n",
    "    for key in ['tag_amount', 'char_amount', 'word_amount', 'max_word_size', 'max_seq_size']:\n",
    "        print(key, params[key])\n",
    "    \n",
    "    \n",
    "    # Creating sequence vectorizer\n",
    "    \n",
    "    sequence_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "        max_tokens = params['word_amount'] + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = params['max_seq_size'],\n",
    "        standardize = params['standardize'],\n",
    "        split = params['split']\n",
    "    )\n",
    "    \n",
    "    sequence_vectorizer.adapt(train_sequences_dataset)\n",
    "    params['sequence_vectorizer'] = sequence_vectorizer\n",
    "    \n",
    "    # Creating tag vectorizer\n",
    "    \n",
    "    tag_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "        max_tokens = params['tag_amount'] + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = params['max_seq_size'],\n",
    "        standardize = None,\n",
    "        split = \"whitespace\"\n",
    "    )\n",
    "    \n",
    "    tag_vectorizer.adapt(train_labels_dataset)\n",
    "    params['tag_vectorizer'] = tag_vectorizer\n",
    "    \n",
    "    # Creating char vectorizer\n",
    "    \n",
    "    char_vectorizer = layers.TextVectorization(\n",
    "        output_mode = \"int\",\n",
    "        max_tokens = params['char_amount'] + 2, # Plus PAD and UNK\n",
    "        output_sequence_length = params['max_word_size'],\n",
    "        standardize = None,\n",
    "        split = \"character\"\n",
    "    )\n",
    "\n",
    "    char_vectorizer.adapt(train_sequences_dataset)\n",
    "    params['char_vectorizer'] = char_vectorizer\n",
    "    \n",
    "    # Adding lookups\n",
    "    word_to_index = dict(map(lambda x: (x[1],x[0]), enumerate(sequence_vectorizer.get_vocabulary())))\n",
    "    params['word_to_index'] = word_to_index\n",
    "    \n",
    "    tag_to_index = dict(map(lambda x: (x[1],x[0]), enumerate(tag_vectorizer.get_vocabulary())))\n",
    "    params['tag_to_index'] = tag_to_index\n",
    "    \n",
    "    char_to_index = dict(map(lambda x: (x[1],x[0]), enumerate(char_vectorizer.get_vocabulary())))\n",
    "    params['char_to_index'] = char_to_index\n",
    "    \n",
    "    for key in ['word_to_index', 'tag_to_index', 'char_to_index']:\n",
    "        print()\n",
    "        print(key)\n",
    "        print('Length:', len(params[key]))\n",
    "        print('First 20:', list(params[key].items())[:20])\n",
    "    \n",
    "add_datasets(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Compute the embedding matrix for the words present in the corpus. The matrix is serialized and saved for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Embedding Matrix Found\n"
     ]
    }
   ],
   "source": [
    "def add_embeddings(params: dict):\n",
    "    if Path(params[\"glove_path\"]).exists():\n",
    "        print(\"Glove Embedding Matrix Found\")\n",
    "        embedding_matrix = np.load(params[\"glove_path\"])[\"embeddings\"]\n",
    "        params['embedding_matrix'] = embedding_matrix\n",
    "        return\n",
    "    \n",
    "    # Loading Glove\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    embedding_dim = params['dim']\n",
    "    word_to_index = params['word_to_index']\n",
    "    num_tokens = len(word_to_index) # Plus padding and unknown \n",
    "\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    with Path(params[\"glove_raw_path\"]).open() as f:\n",
    "        for line_idx, line in enumerate(f):\n",
    "            if line_idx % 100000 == 0:\n",
    "                print('- At line {}'.format(line_idx))\n",
    "            line = line.strip().split()\n",
    "            if len(line) != 300 + 1:\n",
    "                continue\n",
    "            word = line[0]\n",
    "            embedding = line[1:]\n",
    "            if word in word_to_index:\n",
    "                hits += 1\n",
    "                word_idx = word_to_index[word]\n",
    "                embedding_matrix[word_idx] = embedding\n",
    "    print('- Done. Found {} vectors for {} words'.format(hits, num_tokens - 2))\n",
    "    \n",
    "    params['embedding_matrix'] = embedding_matrix\n",
    "    np.savez_compressed(params[\"glove_path\"], embeddings=embedding_matrix)\n",
    "    \n",
    "add_embeddings(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Creates the model based on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 577)]             0         \n",
      "                                                                 \n",
      " embedding_8 (Embedding)     (None, 577, 300)          2702700   \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 577, 400)         801600    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 577, 400)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,504,300\n",
      "Trainable params: 801,600\n",
      "Non-trainable params: 2,702,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(params: dict):\n",
    "    # Common\n",
    "    model_name = params['model_name']\n",
    "    words_amount = params['word_amount'] + 2 # Plus padding and unknown \n",
    "    embedding_dim = params['dim']\n",
    "    embedding_matrix = params['embedding_matrix']\n",
    "    max_sequence_size = params['max_seq_size']\n",
    "    lstm_size = params[\"lstm_size\"]\n",
    "    tag_amount = params['tag_amount'] + 2 # Plus padding and unknown \n",
    "    optimizer = params['optimizer']\n",
    "    metrics = params['metrics']\n",
    "    dropout = params['dropout']\n",
    "    batch_size = params['batch_size']\n",
    "    trainable_word_emeddings = params['trainable_word_emeddings']\n",
    "\n",
    "    # CNN\n",
    "    with_cnn = params['with_cnn']\n",
    "    dim_chars = params[\"dim_chars\"]\n",
    "    filters = params['filters']\n",
    "    kernel_size = params[\"kernel_size\"]\n",
    "    max_word_size = params['max_word_size']\n",
    "    char_amount = params['char_amount'] + 2 # Plus padding and unknown\n",
    "    \n",
    "    # DN\n",
    "    with_dn = params['with_dn']\n",
    "    dense_units = params['dense_units']\n",
    "    dense_activation = params['dense_activation']\n",
    "        \n",
    "    if with_cnn:\n",
    "        \n",
    "        # Input layer char\n",
    "        int_char_input = keras.Input(shape=(max_word_size,), dtype=\"int64\")\n",
    "        \n",
    "        # Embedding layer char\n",
    "        embedding_char_layer = layers.Embedding(\n",
    "            char_amount,\n",
    "            dim_chars,\n",
    "            trainable=True,\n",
    "            input_length=max_word_size,\n",
    "        )\n",
    "\n",
    "        embedded_chars = embedding_char_layer(int_char_input)\n",
    "\n",
    "        # Dropout layer char\n",
    "        model_char_layers = layers.Dropout(dropout)(embedded_chars)\n",
    "\n",
    "        # Convolution Layer char\n",
    "\n",
    "        model_char_layers = layers.Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"same\",\n",
    "            input_shape=(max_word_size, dim_chars),\n",
    "        )(model_char_layers)\n",
    "\n",
    "        # Max Polling layer char\n",
    "\n",
    "        model_char_layers = layers.MaxPooling1D(\n",
    "            max_word_size,\n",
    "            input_shape=(max_word_size, filters),\n",
    "        )(model_char_layers)\n",
    "\n",
    "        # Reshape layer char\n",
    "\n",
    "        model_char_layers = layers.Reshape((filters,), input_shape=(1, filters))(model_char_layers)\n",
    "\n",
    "        char_model = keras.Model(int_char_input, model_char_layers)\n",
    "\n",
    "        char_model.summary()\n",
    "\n",
    "        # Time Distributed with words\n",
    "        inputs_word_chars = keras.Input(\n",
    "            shape=(max_sequence_size, max_word_size), \n",
    "            dtype=\"int64\",\n",
    "        )\n",
    "        model_char_layers = layers.TimeDistributed(\n",
    "            char_model)(inputs_word_chars)\n",
    "\n",
    "        # Dropout layer\n",
    "        model_char_layers = layers.Dropout(dropout)(model_char_layers)\n",
    "\n",
    "    \n",
    "    # Input layer\n",
    "    int_sequences_input = keras.Input(\n",
    "        shape=(max_sequence_size,), \n",
    "        dtype=\"int64\"\n",
    "    )\n",
    "    \n",
    "    # Embedding layer, convert an index vector into a embedding vector, by accessing embedding_matrix\n",
    "    embedding_layer = layers.Embedding(\n",
    "        words_amount,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=trainable_word_emeddings,\n",
    "        input_length=max_sequence_size,\n",
    "    )\n",
    "\n",
    "    model_layers = embedding_layer(int_sequences_input)\n",
    "    \n",
    "    if with_cnn:\n",
    "        # Concatenate char embedding with sentence embedding\n",
    "        model_layers = layers.Concatenate()([model_layers, model_char_layers])\n",
    "        \n",
    "    # BiLSTM Layer\n",
    "    model_layers = layers.Bidirectional(layers.LSTM(\n",
    "        lstm_size, \n",
    "        return_sequences=True,\n",
    "    ))(model_layers)\n",
    "  \n",
    "    # Dropout layer\n",
    "    model_layers = layers.Dropout(dropout)(model_layers)\n",
    "   \n",
    "    if with_dn:\n",
    "        # Dense layer\n",
    "        model_layers = layers.TimeDistributed(\n",
    "            layers.Dense(\n",
    "                dense_units, \n",
    "                activation=dense_activation,\n",
    "            ))(model_layers)\n",
    "\n",
    "        # Dropout layer\n",
    "        model_layers = layers.Dropout(dropout)(model_layers)\n",
    "    \n",
    "    # Model\n",
    "    if with_cnn:\n",
    "        model = keras.Model(\n",
    "            [int_sequences_input, inputs_word_chars], \n",
    "            model_layers\n",
    "        )\n",
    "    else:\n",
    "        model = keras.Model(\n",
    "            int_sequences_input, \n",
    "            model_layers\n",
    "        )\n",
    "        \n",
    "    model.summary()\n",
    "    \n",
    "    # CRF layer\n",
    "    model = tfa.text.crf_wrapper.CRFModelWrapper(\n",
    "        model, \n",
    "        tag_amount)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    params[model_name] = model\n",
    "    \n",
    "    \n",
    "create_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Creates the functions to encode the datasets to perform the training and evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encode_dataset(sequence_dataset, label_dataset, batch_size, with_cnn, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table):\n",
    "    dataset = sequence_dataset.map(lambda sequence: sequence_vectorizer(sequence))\n",
    "    if with_cnn:\n",
    "        dataset = dataset.map(lambda sequence_vec: (sequence_vec, char_vectorizer(index_to_word_table.lookup(sequence_vec))))\n",
    "    if label_dataset is not None:\n",
    "        labels = label_dataset.map(lambda labels: tag_vectorizer(labels))\n",
    "        dataset = tf.data.Dataset.zip((dataset, labels))\n",
    "    if batch_size:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def encode_datasets(params: dict):\n",
    "    sequence_vectorizer = params['sequence_vectorizer']\n",
    "    char_vectorizer = params['char_vectorizer']\n",
    "    tag_vectorizer = params['tag_vectorizer']\n",
    "    with_cnn = params['with_cnn']\n",
    "    word_and_index = [x for x in params['word_to_index'].items()]\n",
    "    tag_and_index = [x for x in params['tag_to_index'].items()]\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    # Creating lookups\n",
    "    keys_tensor = tf.constant([x[1] for x in word_and_index], dtype=\"int64\")\n",
    "    vals_tensor = tf.constant([x[0] for x in word_and_index])\n",
    "    index_to_word_table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
    "        default_value=\"\")\n",
    "    params['index_to_word_table'] = index_to_word_table\n",
    "\n",
    "    keys_tensor = tf.constant([x[1] for x in tag_and_index], dtype=\"int32\")\n",
    "    vals_tensor = tf.constant([x[0] for x in tag_and_index])\n",
    "    index_to_tag_table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
    "        default_value=\"\")\n",
    "    params['index_to_tag_table'] = index_to_tag_table\n",
    "\n",
    "    # Creating datasets\n",
    "    train_sequences = params['train_sequences']\n",
    "    train_labels = params['train_labels']\n",
    "    params['train_ds'] = encode_dataset(train_sequences, train_labels, batch_size, with_cnn, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table)\n",
    "    testa_sequences = params['testa_sequences']\n",
    "    testa_labels = params['testa_labels']\n",
    "    params['testa_ds'] = encode_dataset(testa_sequences, testa_labels, batch_size, with_cnn, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table)\n",
    "    testb_sequences = params['testb_sequences']\n",
    "    train_labels = params['train_labels']\n",
    "    testb_labels = params['testb_labels']\n",
    "    params['testb_ds'] = encode_dataset(testb_sequences, testb_labels, batch_size, with_cnn, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table)\n",
    "    \n",
    "#     print(tf.data.DatasetSpec.from_value(params['train_ds']))\n",
    "#     l = list(params['train_ds'].take(1))\n",
    "#     print(params['train_ds'].element_spec)\n",
    "#     print(l[0][0][0])\n",
    "#     print(l[0][0][1])\n",
    "#     print(l[0][1])\n",
    "    \n",
    "encode_datasets(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the model\n",
    "\n",
    "Perform the model training and serialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph_model_cnn_blstm_crf\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/tensorflow_addons/text/crf_wrapper.py\", line 97, in train_step\n        self.compiled_metrics.update_state(y, decoded_sequence)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 501, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\", line 647, in update_state  **\n        return super(MeanMetricWrapper, self).update_state(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\", line 445, in update_state\n        values, _, sample_weight = losses_utils.squeeze_or_expand_dimensions(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/losses_utils.py\", line 212, in squeeze_or_expand_dimensions\n        sample_weight = tf.squeeze(sample_weight, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 577 for '{{node Squeeze_1}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Cast_7)' with input shapes: [?,577].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Path(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_histories_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_history.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(history\u001b[38;5;241m.\u001b[39mhistory, f)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrain_and_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mtrain_and_save_model\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      8\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_ds\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtestb_ds\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39msave(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], save_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Path(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_histories_path\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_history.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filen30w_2ro.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_addons/text/crf_wrapper.py:97\u001b[0m, in \u001b[0;36mCRFModelWrapper.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Update metrics (includes the metric that tracks the loss)\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoded_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Return a dict mapping metric names to current value\u001b[39;00m\n\u001b[1;32m     99\u001b[0m orig_results \u001b[38;5;241m=\u001b[39m {m\u001b[38;5;241m.\u001b[39mname: m\u001b[38;5;241m.\u001b[39mresult() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics}\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/tensorflow_addons/text/crf_wrapper.py\", line 97, in train_step\n        self.compiled_metrics.update_state(y, decoded_sequence)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 501, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/metrics_utils.py\", line 70, in decorated\n        update_op = update_state_fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\", line 647, in update_state  **\n        return super(MeanMetricWrapper, self).update_state(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/metrics/base_metric.py\", line 445, in update_state\n        values, _, sample_weight = losses_utils.squeeze_or_expand_dimensions(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/losses_utils.py\", line 212, in squeeze_or_expand_dimensions\n        sample_weight = tf.squeeze(sample_weight, [-1])\n\n    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 577 for '{{node Squeeze_1}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](Cast_7)' with input shapes: [?,577].\n"
     ]
    }
   ],
   "source": [
    "def train_and_save_model(params: dict):\n",
    "\n",
    "    model_name = params['model_name']\n",
    "    print(model_name)\n",
    "    epochs = params['epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    model = params[model_name]\n",
    "    train_ds = params['train_ds']\n",
    "    val_ds = params['testb_ds']\n",
    "    history = model.fit(train_ds,\n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  validation_data=val_ds)\n",
    "    \n",
    "    model.save(params[\"model_path\"], save_format='tf')\n",
    "    with Path(params['model_histories_path'], f\"{model_name}_history.json\").open('w') as f:\n",
    "        json.dump(history.history, f)\n",
    "    \n",
    "\n",
    "train_and_save_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(params: dict):\n",
    "    \n",
    "    title = model_name = params[\"model_name\"]\n",
    "    \n",
    "    base_path = Path(params['model_histories_path'])\n",
    "    path = base_path / f\"{model_name}_history.json\"\n",
    "    \n",
    "    history = json.load(path.open())\n",
    "    print(history.keys())\n",
    "    \n",
    "    plt.plot(history['loss'], label='Cross entropy loss train')\n",
    "    plt.plot(history['val_loss'], label='Cross entropy loss validation')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Cross entropy value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history['acc'], label='Accuracy train')\n",
    "    plt.plot(history['val_acc'], label='Accuracy validation')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "plot_history(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model(params: dict, path_key: str, param_save_key: str):\n",
    "    model = keras.models.load_model(params[path_key])\n",
    "    params[param_save_key] = model\n",
    "    \n",
    "    \n",
    "load_saved_model(params, 'model_path', params['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(params: dict):\n",
    "    \n",
    "    model_name = params['model_name']\n",
    "    batch_size = params['batch_size']\n",
    "    model = params[model_name]\n",
    "    test_ds = params['testa_ds']\n",
    "    \n",
    "    results = model.evaluate(test_ds, batch_size=batch_size)\n",
    "    print(\"test acc, test loss\", results)\n",
    "\n",
    "evaluate_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models to Export\n",
    "\n",
    "Creates models that are easier to use by performing encoding and decoding to the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def decode_output(output, index_to_tag_table):\n",
    "    return tf.map_fn(lambda x: index_to_tag_table.lookup(x), output, fn_output_signature=tf.string)\n",
    "\n",
    "class ExportModel(keras.Model):\n",
    "    \n",
    "    def __init__(self, model, with_cnn, sequence_vectorizer, tag_vectorizer, char_vectorizer, index_to_word_table, index_to_tag_table):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.with_cnn = with_cnn\n",
    "        self.sequence_vectorizer = sequence_vectorizer\n",
    "        self.tag_vectorizer = tag_vectorizer\n",
    "        self.char_vectorizer = char_vectorizer\n",
    "        self.index_to_word_table = index_to_word_table\n",
    "        self.index_to_tag_table = index_to_tag_table\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_tensors = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "        encoded_inputs = encode_dataset(input_tensors, None, 0, self.with_cnn, self.sequence_vectorizer, self.tag_vectorizer, self.char_vectorizer, self.index_to_word_table)\n",
    "        \n",
    "        if self.with_cnn:\n",
    "            word_encoded_inputs = encoded_inputs.map(lambda x,y: x)\n",
    "            char_encoded_inputs = encoded_inputs.map(lambda x,y: y)\n",
    "            word_encoded_inputs = tf.convert_to_tensor(list(word_encoded_inputs))\n",
    "            char_encoded_inputs = tf.convert_to_tensor(list(char_encoded_inputs))\n",
    "            results = self.model([word_encoded_inputs, char_encoded_inputs])\n",
    "        else:\n",
    "            word_encoded_inputs = tf.convert_to_tensor(list(encoded_inputs))\n",
    "            results = self.model(word_encoded_inputs)\n",
    "\n",
    "        decoded_output = decode_output(results, self.index_to_tag_table)\n",
    "        return decoded_output\n",
    "    \n",
    "def save_final_model(params: dict):\n",
    "    model_name = params['model_name']\n",
    "    model = params[model_name]\n",
    "    with_cnn = params['with_cnn']\n",
    "    sequence_vectorizer = params['sequence_vectorizer']\n",
    "    tag_vectorizer = params['tag_vectorizer']\n",
    "    char_vectorizer = params['char_vectorizer']\n",
    "    index_to_word_table = params['index_to_word_table'] \n",
    "    index_to_tag_table = params['index_to_tag_table']\n",
    "    \n",
    "    full_model = ExportModel(\n",
    "        model, \n",
    "        with_cnn,\n",
    "        sequence_vectorizer,\n",
    "        tag_vectorizer,\n",
    "        char_vectorizer,\n",
    "        index_to_word_table,\n",
    "        index_to_tag_table,\n",
    "    )\n",
    "    \n",
    "    inputs = tf.constant([\n",
    "        \"No podemos obligar a poner el mismo n√∫mero de hombres y mujeres en todas las materias Existe la opini√≥n de que las universidades y los colegios deber√≠an matricular por igual a estudiantes hombres y mujeres en cada facultad . Personalmente , no estoy de acuerdo con el punto de vista , porque existen muchos caracteres diferentes entre estudiantes y estudiantes . Por un lado , los ni√±os y ni√±as tienen diversidad en modos psicol√≥gicos e individualidad . La mayor√≠a de los estudiantes varones tienden a utilizar el lado izquierdo del cerebro para pensar y actuar , y en muchos casos son m√°s racionales y l√≥gicos que las ni√±as . Por ejemplo , hay m√°s cient√≠ficos e ingenieros hombres en comparaci√≥n con las mujeres en todo el mundo . Muchos ni√±os est√°n interesados ‚Äã‚Äãen la ciencia y la tecnolog√≠a , mientras que a varias ni√±as les gusta aprender literatura , educaci√≥n y artes . Adem√°s , es m√°s probable que las chicas prefieran algunos trabajos relacionados con la emoci√≥n y la comunicaci√≥n , como profesora , cantante e int√©rprete . Esto significa que las ni√±as difieren en gran medida de los ni√±os en mente y comportamiento , y ambos tienen mejores habilidades en el aspecto espec√≠fico . Adem√°s , puede tener un efecto negativo en estos estudiantes exigirles que elijan una materia en igual proporci√≥n de g√©nero , y que no se ajuste a los rasgos de personalidad y desarrollo mental de los estudiantes . Por ejemplo , una ni√±a que est√° interesada en la literatura es asignada a un departamento de ingenier√≠a , pero es poco probable que se concentre en su materia , y esto tambi√©n puede bloquear el futuro desarrollo y la perspectiva profesional de la ni√±a . Por otro lado , las universidades deber√≠an animar a m√°s chicas a elegir asignaturas de ciencias ya m√°s chicos a estudiar humanidades , y esto podr√≠a evitar desequilibrios de g√©nero en algunas asignaturas . Afectar√≠a la salud mental de los estudiantes estudiar en un ambiente de un solo g√©nero . En conclusi√≥n , es necesario que las universidades respeten la elecci√≥n individual de asignaturas debido a la diversidad de chicos y chicas , y no podemos obligar a poner el mismo n√∫mero de chicos y chicas en todas las asignaturas .\"\n",
    "    ])\n",
    "    result = full_model(inputs)\n",
    "    print(result)\n",
    "    \n",
    "    params[\"full_model\"] = full_model\n",
    "#     full_model.save(params[\"full_model_path\"], save_format=\"tf\")\n",
    "\n",
    "save_final_model(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(tag_fixer)\n",
    "import tag_fixer\n",
    "from tag_fixer import fix_tags\n",
    "\n",
    "class ExportFixedTagsModel(ExportModel):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        result = super().call(inputs)\n",
    "        fixed_result = [fix_tags([s.numpy().decode() for s in x if s.numpy().decode()], verbose=False) for x in result]\n",
    "        return fixed_result\n",
    "\n",
    "def fixed_tags_model(params: dict):\n",
    "    model_name = params['model_name']\n",
    "    model = params[model_name]\n",
    "    with_cnn = params['with_cnn']\n",
    "    sequence_vectorizer = params['sequence_vectorizer']\n",
    "    tag_vectorizer = params['tag_vectorizer']\n",
    "    char_vectorizer = params['char_vectorizer']\n",
    "    index_to_word_table = params['index_to_word_table'] \n",
    "    index_to_tag_table = params['index_to_tag_table']\n",
    "    \n",
    "    full_model = ExportFixedTagsModel(\n",
    "        model, \n",
    "        with_cnn,\n",
    "        sequence_vectorizer,\n",
    "        tag_vectorizer,\n",
    "        char_vectorizer,\n",
    "        index_to_word_table,\n",
    "        index_to_tag_table,\n",
    "    )\n",
    "    \n",
    "    inputs = tf.constant([\n",
    "        \"No podemos obligar a poner el mismo n√∫mero de hombres y mujeres en todas las materias Existe la opini√≥n de que las universidades y los colegios deber√≠an matricular por igual a estudiantes hombres y mujeres en cada facultad . Personalmente , no estoy de acuerdo con el punto de vista , porque existen muchos caracteres diferentes entre estudiantes y estudiantes . Por un lado , los ni√±os y ni√±as tienen diversidad en modos psicol√≥gicos e individualidad . La mayor√≠a de los estudiantes varones tienden a utilizar el lado izquierdo del cerebro para pensar y actuar , y en muchos casos son m√°s racionales y l√≥gicos que las ni√±as . Por ejemplo , hay m√°s cient√≠ficos e ingenieros hombres en comparaci√≥n con las mujeres en todo el mundo . Muchos ni√±os est√°n interesados ‚Äã‚Äãen la ciencia y la tecnolog√≠a , mientras que a varias ni√±as les gusta aprender literatura , educaci√≥n y artes . Adem√°s , es m√°s probable que las chicas prefieran algunos trabajos relacionados con la emoci√≥n y la comunicaci√≥n , como profesora , cantante e int√©rprete . Esto significa que las ni√±as difieren en gran medida de los ni√±os en mente y comportamiento , y ambos tienen mejores habilidades en el aspecto espec√≠fico . Adem√°s , puede tener un efecto negativo en estos estudiantes exigirles que elijan una materia en igual proporci√≥n de g√©nero , y que no se ajuste a los rasgos de personalidad y desarrollo mental de los estudiantes . Por ejemplo , una ni√±a que est√° interesada en la literatura es asignada a un departamento de ingenier√≠a , pero es poco probable que se concentre en su materia , y esto tambi√©n puede bloquear el futuro desarrollo y la perspectiva profesional de la ni√±a . Por otro lado , las universidades deber√≠an animar a m√°s chicas a elegir asignaturas de ciencias ya m√°s chicos a estudiar humanidades , y esto podr√≠a evitar desequilibrios de g√©nero en algunas asignaturas . Afectar√≠a la salud mental de los estudiantes estudiar en un ambiente de un solo g√©nero . En conclusi√≥n , es necesario que las universidades respeten la elecci√≥n individual de asignaturas debido a la diversidad de chicos y chicas , y no podemos obligar a poner el mismo n√∫mero de chicos y chicas en todas las asignaturas .\"\n",
    "    ])\n",
    "    result = full_model(inputs)\n",
    "    print(result)\n",
    "    params[\"full_fixed_model\"] = full_model\n",
    "\n",
    "fixed_tags_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing statistics about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def models_statistics(params: dict):\n",
    "    testa_sequences_dataset = params[\"testa_sequences\"] \n",
    "    testa_labels_dataset = params[\"testa_labels\"]\n",
    "    batch_size = params['batch_size']\n",
    "    model = params['full_model']\n",
    "    fixed_model = params['full_fixed_model']\n",
    "    \n",
    "    general_statistic = {\n",
    "        \"Word\": [],\n",
    "        \"TrueTag\": [],\n",
    "        \"InferedTag\": [],\n",
    "        \"FixedInferedTag\": []\n",
    "    }\n",
    "    \n",
    "    total_size = 0\n",
    "    \n",
    "    for test_seq_batch, test_lbl_batch in tf.data.Dataset.zip((testa_sequences_dataset, testa_labels_dataset)).batch(batch_size):\n",
    "        infered_test_lbl_batch = model(test_seq_batch)\n",
    "        infered_test_lbl_fixed_batch = fixed_model(test_seq_batch)\n",
    "        \n",
    "        for words, true_labels, infered_labels, fixed_infered_labels in zip(test_seq_batch, test_lbl_batch, infered_test_lbl_batch, infered_test_lbl_fixed_batch):\n",
    "            words = words.numpy().decode().split()\n",
    "            true_labels = true_labels.numpy().decode().split()\n",
    "            \n",
    "            for word, true_label, infered_label, fixed_infered_label in zip(words, true_labels, infered_labels, fixed_infered_labels):\n",
    "                infered_label = infered_label.numpy().decode()\n",
    "                \n",
    "                if true_label != infered_label \\\n",
    "                or true_label != fixed_infered_label \\\n",
    "                or infered_label != fixed_infered_label:\n",
    "                    general_statistic[\"Word\"].append(word)\n",
    "                    general_statistic[\"TrueTag\"].append(true_label)\n",
    "                    general_statistic[\"InferedTag\"].append(infered_label)\n",
    "                    general_statistic[\"FixedInferedTag\"].append(fixed_infered_label)\n",
    "                    \n",
    "                total_size += 1\n",
    "            \n",
    "    general_statistic = pd.DataFrame(general_statistic)\n",
    "    params['general_statistic'] = (general_statistic, total_size)\n",
    "    \n",
    "models_statistics(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_statistic(params: dict):\n",
    "    general_statistic, total_size = params['general_statistic']\n",
    "    \n",
    "    print(general_statistic.describe())\n",
    "    \n",
    "    infered_errors = general_statistic[general_statistic[\"TrueTag\"] != general_statistic[\"InferedTag\"]]\n",
    "    infered_fixed_errors = general_statistic[general_statistic[\"TrueTag\"] != general_statistic[\"FixedInferedTag\"]]\n",
    "    infered_fixes = general_statistic[general_statistic[\"InferedTag\"] != general_statistic[\"FixedInferedTag\"]]\n",
    "    bio_infered_errors = general_statistic[general_statistic[\"TrueTag\"].map(lambda x: x[0]) != general_statistic[\"InferedTag\"].map(lambda x: x[0])]\n",
    "    bio_infered_fix_errors = general_statistic[general_statistic[\"TrueTag\"].map(lambda x: x[0]) != general_statistic[\"FixedInferedTag\"].map(lambda x: x[0])]\n",
    "    bio_infered_fixes = general_statistic[general_statistic[\"InferedTag\"].map(lambda x: x[0]) != general_statistic[\"FixedInferedTag\"].map(lambda x: x[0])]\n",
    "    \n",
    "    headers = [\n",
    "        (\"General Error Rate\", general_statistic),\n",
    "        (\"Infered Error Rate\", infered_errors),\n",
    "        (\"Infered with Fixed Tags Error Rate\", infered_fixed_errors),\n",
    "        (\"BIOES-only Infered Error Rate\", bio_infered_errors),\n",
    "        (\"BIOES-only Infered with Fixed Tags Error Rate\", bio_infered_fix_errors),\n",
    "    ]\n",
    "    print()\n",
    "    for title, df in headers:\n",
    "        print(title)\n",
    "        print(len(df)/total_size)\n",
    "        print()\n",
    "    \n",
    "show_statistic(params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model\n",
    "\n",
    "Perform the segmentation on text. The data to process directory must contain folders with **.txt** files, the tokens in this files must be separated by whitepaces. This files will be passed through the model and the output will be written in conll format in the processed directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def perform_segmentation(params: dict):\n",
    "    model = params['full_fixed_model']\n",
    "    batch_size = params['batch_size']\n",
    "    to_process_path = params['to_process_data_path']\n",
    "    to_save_path = params['processed_data_path']\n",
    "    \n",
    "    labels = sorted(list(os.walk(to_process_path))[0][1])\n",
    "    print(labels)\n",
    "    files = keras.utils.text_dataset_from_directory(\n",
    "        str(to_process_path), \n",
    "        class_names = labels,\n",
    "        shuffle = False,\n",
    "    )\n",
    "    \n",
    "    base_path = Path(to_save_path) / params['model_name']\n",
    "    base_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    file_number = 0\n",
    "    filenames = [file.name for file in Path(to_process_path).rglob('*.txt')]\n",
    "    \n",
    "    for text_batch, label_batch in files:\n",
    "        tag_batch = model(text_batch)\n",
    "        for text, label, tags in zip(text_batch, label_batch, tag_batch):\n",
    "            text = text.numpy().decode().split()\n",
    "            tags = [tag for tag in tags]\n",
    "            \n",
    "            current_file = base_path / labels[label]\n",
    "            current_file.mkdir(exist_ok=True)\n",
    "            current_file /= filenames[file_number]\n",
    "            current_file.touch()\n",
    "            current_file.write_text(\"\\n\".join(f\"{word}\\t{tag}\" for word,tag in zip(text, tags)))\n",
    "            file_number += 1\n",
    "            \n",
    "\n",
    "perform_segmentation(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
